{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 15:58:55.422777: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-18 15:58:55.441854: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-18 15:58:55.441874: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-18 15:58:55.442343: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-18 15:58:55.445819: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-18 15:58:55.914561: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from music21 import *\n",
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim         \n",
    "import tensorflow as tf\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Preprocessed Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Read data from CSV files\n",
    "notes_df = pd.read_csv('/home/admin1/Desktop/NEWLSTM/csv_dataset/notes.csv')\n",
    "test_df = pd.read_csv('/home/admin1/Desktop/NEWLSTM/csv_dataset/testset.csv')\n",
    "\n",
    "# Extract data and labels from the test set\n",
    "data_test = test_df[['x_test', 'future']].to_numpy()\n",
    "x_test_string = data_test[:, 0]\n",
    "y_test_string = data_test[:, 1]\n",
    "\n",
    "# Convert string representations to lists\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in x_test_string:\n",
    "    # Remove square brackets and newline characters\n",
    "    i = i.strip('[]\\n')\n",
    "    # Split the string and convert elements to integers\n",
    "    input_x_test = [int(j) for j in i.split()]\n",
    "    x_test.append(input_x_test)\n",
    "\n",
    "for i in y_test_string:\n",
    "    # Remove square brackets and newline characters\n",
    "    i = i.strip('[]\\n')\n",
    "    # Split the string and convert elements to integers\n",
    "    input_y_test = [int(j) for j in i.split()]\n",
    "    y_test.append(input_y_test)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Create a dictionary of unique notes\n",
    "notes_ = notes_df.iloc[:, 0].to_numpy()\n",
    "unique_notes = dict(enumerate(notes_.flatten(), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, data , data_labels):\n",
    "        self.data = data\n",
    "        self.data_labels = data_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx] , self.data_labels[idx]\n",
    "\n",
    "\n",
    "test_set = MusicDataset(x_test,y_test)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=1,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('embedding.weight', tensor([[-0.9660, -0.1753, -0.9998,  ..., -2.4478,  0.7413,  0.1131],\n",
      "        [ 1.1218, -0.3555, -1.0031,  ...,  0.2686,  0.4533, -1.3828],\n",
      "        [ 0.6009,  0.6317,  1.8816,  ..., -0.2737, -0.3908,  1.7856],\n",
      "        ...,\n",
      "        [ 0.0391,  2.5164,  2.5064,  ...,  0.6519, -0.4782, -0.4989],\n",
      "        [-0.2807,  0.6004,  1.6323,  ...,  0.9294,  0.1259,  0.5602],\n",
      "        [-0.0171, -0.7937,  0.0736,  ..., -1.7346, -1.5824, -2.2183]])), ('lstm.weight_ih_l0', tensor([[ 0.2736, -0.5099,  0.2236,  ..., -0.1463,  1.0522, -0.2698],\n",
      "        [ 0.0973,  0.1648,  0.4945,  ..., -0.2400,  0.9486, -0.4667],\n",
      "        [ 0.3904,  0.2224, -0.2705,  ...,  0.0825,  0.2308, -0.8683],\n",
      "        ...,\n",
      "        [-0.0575, -0.3865, -0.0631,  ..., -0.0593,  0.0908, -0.1832],\n",
      "        [ 0.3537, -0.2953, -0.4213,  ..., -0.6575,  0.0952,  0.4562],\n",
      "        [-0.1336,  0.3779, -0.3101,  ...,  1.0342,  0.2886,  0.1842]])), ('lstm.weight_hh_l0', tensor([[ 0.2977,  0.0255, -0.7435,  ..., -0.1524, -0.8160, -0.3825],\n",
      "        [-0.7505,  0.3289,  0.3222,  ...,  0.0107, -0.2896, -0.2474],\n",
      "        [-0.0731, -0.2589,  0.5669,  ..., -0.9718, -0.0980, -0.6057],\n",
      "        ...,\n",
      "        [-0.3679, -0.0388,  0.0229,  ...,  0.2885, -0.0099, -0.3419],\n",
      "        [ 0.3927,  0.0748,  0.1280,  ..., -1.0317, -0.1410,  0.1175],\n",
      "        [ 0.4516, -0.8382, -0.2188,  ..., -0.3379, -0.3589,  0.3568]])), ('lstm.bias_ih_l0', tensor([-0.5117, -0.8269, -0.2864,  ..., -0.9292, -0.4602, -0.1530])), ('lstm.bias_hh_l0', tensor([-0.5291, -0.8679, -0.3788,  ..., -0.9100, -0.4711, -0.1353])), ('lstm.weight_ih_l1', tensor([[-0.5360,  0.1027,  0.6437,  ..., -0.2988, -0.4150, -0.7934],\n",
      "        [ 0.0934, -0.2870, -0.3428,  ..., -0.1762, -0.7330, -0.0530],\n",
      "        [ 0.6371, -0.4738, -1.2774,  ..., -0.3640,  0.3888, -0.5627],\n",
      "        ...,\n",
      "        [ 1.3862, -0.4073, -0.8273,  ..., -1.1403,  0.5008,  0.2483],\n",
      "        [ 0.3670, -0.0807,  0.1273,  ..., -0.1322, -0.3804,  0.3473],\n",
      "        [ 0.2619, -0.6718,  0.3924,  ...,  0.4005, -0.3864, -0.1297]])), ('lstm.weight_hh_l1', tensor([[ 0.3080, -1.1985,  0.6412,  ...,  0.5676, -0.9024,  0.4947],\n",
      "        [-0.9325,  0.2428,  0.1894,  ...,  0.0461,  1.0692,  0.2119],\n",
      "        [ 0.2108,  0.2218,  0.5367,  ..., -0.4587, -0.4496, -0.3103],\n",
      "        ...,\n",
      "        [-0.7477,  0.3010,  0.5121,  ...,  1.0011,  0.4959, -0.6427],\n",
      "        [-0.7061, -0.4891, -0.2096,  ...,  0.7248, -0.1045, -0.3788],\n",
      "        [ 1.3129,  0.3715,  0.3707,  ..., -0.0779, -0.8854, -0.8004]])), ('lstm.bias_ih_l1', tensor([-0.5296, -0.9579, -0.6520,  ..., -0.5313, -0.1823, -0.5171])), ('lstm.bias_hh_l1', tensor([-0.5403, -0.9860, -0.7262,  ..., -0.5701, -0.1716, -0.5112])), ('fc1.weight', tensor([[ 1.0674, -0.1236, -0.0711,  ...,  0.3585,  0.0383,  0.3224],\n",
      "        [ 0.0386,  0.0105,  0.0552,  ..., -0.6721,  0.1783, -0.2961],\n",
      "        [ 0.7906,  0.0595,  0.4900,  ..., -0.3196, -0.0237, -0.0857],\n",
      "        ...,\n",
      "        [ 0.4438,  0.3918, -0.7731,  ...,  0.4928, -0.4016,  0.3116],\n",
      "        [ 0.3009, -0.1562, -0.2251,  ...,  1.3094, -0.2431,  0.2996],\n",
      "        [-0.4054, -0.4541,  0.3168,  ...,  0.1039,  0.7135,  0.4598]])), ('fc1.bias', tensor([-2.1756, -1.4709, -1.0061, -0.5723, -0.9762, -0.6812, -0.8292, -1.6875,\n",
      "        -0.5267, -0.5462, -1.7243, -1.3515, -0.7943, -1.4270, -1.3072, -1.5884,\n",
      "        -1.5006, -1.4901, -0.9616, -1.4257, -0.9114, -0.1307, -0.4979, -0.6719,\n",
      "        -1.1526, -0.7768, -0.9781, -1.0760, -0.6968, -1.3819, -0.5194, -1.8136,\n",
      "        -2.0434, -1.5922, -1.8734, -0.8801, -0.9736, -0.3973, -1.1376, -1.2019,\n",
      "        -0.8033, -1.9387, -1.4137, -0.3521, -0.2669, -1.6935, -0.5071, -2.1979,\n",
      "        -1.9375, -1.3196, -1.7794, -1.5612, -1.1433, -1.3821, -1.4934, -2.1579,\n",
      "        -0.5989, -0.8906, -0.3451, -2.2908, -1.3166, -1.0012, -1.5692, -1.8600,\n",
      "        -0.5613, -1.4271, -0.7318, -0.5302, -1.9717, -2.1221, -1.9419, -0.7410,\n",
      "        -1.9215, -0.6248, -1.3602, -1.6976, -1.5908, -1.0748, -0.6601, -0.3176,\n",
      "        -1.6765, -0.8496, -1.5411, -1.5113, -1.9525, -1.5441, -0.5573, -1.3236,\n",
      "        -0.9593, -1.3165, -1.9873, -1.5777, -0.8554, -0.9662, -1.5928, -0.6134,\n",
      "        -0.7960, -1.6214, -0.2974, -0.6605, -0.8772, -0.9540, -1.1407, -0.5869,\n",
      "        -1.4654, -0.2819, -1.3743, -1.1614, -1.0536, -1.6909, -1.8743, -0.5816,\n",
      "        -2.0175, -1.1626, -0.5734, -0.9085, -0.7689, -1.1487, -0.8689, -0.5522,\n",
      "        -1.7123, -0.5774, -1.7039, -1.6450, -1.8094, -1.5425, -1.4449, -1.7548])), ('fc2.weight', tensor([[ -3.3196,  -4.6927,  -7.7082,  ...,  -3.7813,  -2.9431,  -4.9501],\n",
      "        [ -9.4430,  -2.2490,  -5.2954,  ...,  -3.6400,  -5.2266,  -5.7875],\n",
      "        [ -6.2997, -13.3991,  -6.1138,  ...,  -6.7116,  -7.5593,  -7.3295],\n",
      "        ...,\n",
      "        [-11.5332,  -9.6357, -14.2145,  ...,  -2.5583,  -7.6086, -10.2283],\n",
      "        [ -7.2086,  -2.1884,  -0.7889,  ...,  -2.4954,  -2.5589,  -3.2369],\n",
      "        [ -7.0670,  -6.3149,  -6.8225,  ...,  -7.5634,  -5.5420,  -5.9392]])), ('fc2.bias', tensor([-1.1266e+00, -1.2244e+00,  7.3386e-02, -6.2304e-01, -7.7298e-02,\n",
      "         1.9772e-01, -8.7152e-01,  2.4133e-02, -8.5769e-01, -1.0663e+00,\n",
      "        -1.5101e+00,  1.1296e+00, -9.8978e-01, -5.4427e-01, -9.3570e-01,\n",
      "         3.0408e-01, -3.3887e-01, -1.5612e-01, -1.6868e-01, -1.3137e+00,\n",
      "         1.0277e+00, -5.3033e-01, -6.3710e-01, -9.8882e-01,  4.0054e-01,\n",
      "        -6.2909e-01,  4.0906e-01, -4.2211e-01,  3.2376e-01, -1.1367e+00,\n",
      "        -1.3383e+00, -6.6836e-01, -8.6289e-01,  1.3127e+00, -4.3309e-01,\n",
      "        -2.9458e-01,  4.5846e-02,  4.1368e-01,  4.0988e-02, -8.9528e-01,\n",
      "         8.3643e-01, -1.2145e+00, -4.1793e-01,  3.5923e-01, -9.6076e-01,\n",
      "        -2.4386e-01, -7.8266e-01,  1.6409e-01, -1.3894e+00, -1.6183e+00,\n",
      "         1.7373e+00,  9.8036e-01, -1.1327e+00, -2.4199e-01, -3.2847e-01,\n",
      "        -5.6473e-01, -3.8344e-01, -9.0906e-01, -2.2578e-01,  1.5368e+00,\n",
      "         2.0578e-01, -6.0688e-03,  1.5708e+00,  6.9434e-01, -1.0178e+00,\n",
      "        -8.2716e-01, -8.2891e-01, -9.3094e-01,  4.2347e-01, -9.9726e-01,\n",
      "         2.1612e-01, -9.1897e-01, -1.0918e-01,  8.3201e-02,  6.6601e-01,\n",
      "        -3.7056e-01,  1.0916e-01,  1.6467e+00,  1.2440e+00,  3.2627e-01,\n",
      "        -5.3674e-01,  6.3567e-01, -9.4595e-01, -4.8294e-01, -1.2688e+00,\n",
      "         5.4223e-01, -5.2810e-01,  4.4868e-01,  7.5857e-01,  8.1704e-01,\n",
      "        -8.3639e-01, -1.0458e+00, -5.9117e-01,  4.7793e-01, -5.8000e-01,\n",
      "         1.2276e+00, -4.3351e-01, -9.8518e-02, -6.0751e-01, -5.0551e-01,\n",
      "        -1.4642e+00,  2.9154e-01, -1.2303e+00, -4.5647e-01,  1.8304e-01,\n",
      "        -9.6374e-01, -1.3320e+00,  9.6297e-01, -1.2893e+00,  1.8232e+00,\n",
      "         1.7968e+00,  1.3554e+00, -1.4971e+00, -6.8894e-01, -3.1527e-01,\n",
      "        -7.7681e-01,  3.3876e-01, -1.1441e+00,  5.5786e-01, -3.1708e-01,\n",
      "         6.1738e-02, -5.3134e-01, -1.3587e+00, -2.5409e-01, -7.1175e-01,\n",
      "        -1.0849e+00, -1.3613e+00,  7.4026e-01, -3.0219e-01, -9.7603e-02,\n",
      "        -8.1851e-01, -9.4166e-01,  3.6422e-01, -1.0473e+00, -7.8531e-01,\n",
      "         7.7085e-01,  3.5860e-01, -6.4705e-01, -8.0438e-01, -1.9258e-01,\n",
      "         1.1266e+00, -1.0671e+00, -8.3191e-01, -1.2553e+00, -6.7611e-01,\n",
      "        -1.6231e-01,  6.9063e-02,  1.0133e+00, -2.3241e-01,  2.5448e-01,\n",
      "         7.8301e-01,  4.7860e-01, -3.3006e-01, -7.4873e-01,  1.2731e+00,\n",
      "        -1.3250e+00, -1.1953e+00, -9.1545e-01,  8.0083e-01, -1.2864e+00,\n",
      "        -1.1362e-01,  8.3859e-01, -6.3196e-01, -6.8179e-01, -1.1034e+00,\n",
      "         3.2134e-01,  7.2881e-01, -7.7493e-01, -7.3217e-01,  1.5547e-01,\n",
      "         1.3495e-03, -1.3780e+00, -8.7712e-01, -1.2290e-01, -2.1128e-01,\n",
      "        -1.0939e+00, -6.4917e-01,  4.2705e-01, -8.6868e-01, -2.5284e-01,\n",
      "        -5.2503e-01, -4.0725e-01, -3.0826e-02, -1.1673e+00, -9.3520e-01,\n",
      "        -5.6128e-01, -9.8352e-01,  2.3307e-01, -9.6631e-01,  9.9703e-01,\n",
      "         3.1813e-01, -8.9504e-01, -1.5096e+00, -2.0668e-01, -1.0573e+00,\n",
      "         1.6159e-01,  8.2901e-01, -3.1275e-01,  2.1454e-01, -5.9709e-01,\n",
      "        -3.0899e-01, -8.9733e-01,  4.8150e-01, -1.1380e-02,  9.2418e-01,\n",
      "        -1.3085e+00,  1.0322e-01, -1.6860e-01, -1.5203e+00, -2.7295e-01,\n",
      "        -1.0897e+00, -6.7310e-01,  1.2097e+00,  3.5461e-01, -1.0687e+00,\n",
      "        -8.3012e-01,  1.2442e+00, -4.7568e-01, -8.2049e-01, -6.8475e-01,\n",
      "        -7.9688e-01, -1.0186e+00, -5.0861e-01, -6.3516e-01,  2.8519e-01,\n",
      "        -6.2664e-01, -9.5618e-02, -1.5973e-01,  1.7627e-02,  8.5401e-01,\n",
      "        -1.2849e+00, -1.2327e+00, -2.0507e-01, -1.0395e+00, -1.0656e+00,\n",
      "        -8.7666e-01, -8.9717e-01, -4.7928e-01, -7.0741e-01,  2.7443e-01,\n",
      "        -9.0573e-02,  1.2626e+00, -1.1240e+00, -9.7863e-01, -2.3549e-02,\n",
      "         5.5795e-01,  6.4157e-01,  6.7529e-01, -7.0271e-02,  3.8574e-01,\n",
      "        -7.5847e-01, -3.4548e-01,  1.4159e-01, -1.0633e+00, -6.4280e-01,\n",
      "        -1.0419e+00, -1.1163e-01, -1.7990e-02,  8.0003e-01, -5.7219e-01,\n",
      "         4.0318e-01, -6.8972e-01,  1.6546e+00, -1.1208e+00, -3.6623e-01,\n",
      "         8.3106e-01, -6.7165e-01, -6.2951e-01, -5.9211e-01, -1.3241e+00,\n",
      "        -9.7383e-01, -2.9359e-01, -9.7850e-01, -9.0310e-01,  8.1275e-01,\n",
      "        -1.8948e-02, -4.8327e-01,  3.7985e-01, -7.9256e-01, -1.2917e+00,\n",
      "        -6.5371e-01,  1.9895e-01, -1.4157e-01, -4.5521e-01,  2.4140e-01,\n",
      "         1.0788e+00,  1.5198e-01, -7.3353e-01,  4.3231e-01,  4.0882e-01,\n",
      "        -6.1317e-01, -6.1637e-01, -1.1039e+00,  3.7829e-01, -6.2016e-01,\n",
      "        -5.4423e-01,  6.2612e-01, -4.3882e-01,  5.9009e-02, -1.4086e+00,\n",
      "         1.0041e+00]))])\n"
     ]
    }
   ],
   "source": [
    "Net = torch.load('/home/admin1/Desktop/NEWLSTM/state_dictionary/lstmmodel_basic.pth')\n",
    "print(Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the same model as that in Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_classes, 100)\n",
    "        self.lstm = nn.LSTM(input_size=100, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Take the last timestep's output\n",
    "        x = self.fc1(lstm_out)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model based on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy : \n",
      " correct predictions  : 6823 \n",
      " total predictions : 108568 \n",
      " Testing Accuracy : 6.284540564438877 \n",
      " ------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_classes = 301\n",
    "seq_length = 100\n",
    "\n",
    "model = LSTM(num_classes)  # Instantiate your model\n",
    "model.load_state_dict(torch.load('/home/admin1/Desktop/NEWLSTM/state_dictionary/lstmmodel_basic.pth'))  # Load the state dictionary\n",
    "\n",
    "# Define testloader\n",
    "testloader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "total_preds = 0\n",
    "correct_preds = 0\n",
    "future_preds = 8\n",
    "\n",
    "for i, data in enumerate(testloader, 0):\n",
    "    input, label = data\n",
    "    cumm_output = torch.zeros(0, len(unique_notes)).to(device)\n",
    "    cumm_label = np.array([], dtype=int)\n",
    "    \n",
    "    for k in range(future_preds):\n",
    "        # Perform inference using the model\n",
    "        output = model(input.to(device))\n",
    "        cumm_output = torch.cat((cumm_output, output))\n",
    "        cumm_label = np.concatenate((cumm_label, label[:, k]))\n",
    "        next_preds = np.argmax(output.cpu().detach().numpy(), axis=1)\n",
    "        total_preds += input.shape[0]\n",
    "        correct_preds += torch.sum(torch.argmax(output, 1) == label[:, k].to(device))\n",
    "        input = input.cpu().detach().numpy()\n",
    "        input = torch.from_numpy(np.array([np.append(j, next_preds[ind]) \n",
    "                                           for ind, j in enumerate(input)])[:, 1:])\n",
    "\n",
    "test_acc = float(correct_preds) / float(total_preds) * 100\n",
    "testreport = \"Testing Accuracy : \\n correct predictions  : {} \\n total predictions : {} \\n Testing Accuracy : {} \\n ------------------------\\n\".format(correct_preds, total_preds, test_acc)\n",
    "print(testreport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the generated output in the form of MIDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def note_to_int(note):\n",
    "    # Dictionary mapping note names to MIDI numbers\n",
    "    note_dict = {\n",
    "        'C': 0, 'C#': 1, 'Db': 1, 'D': 2, 'D#': 3, 'Eb': 3, 'E': 4, 'F': 5,\n",
    "        'F#': 6, 'Gb': 6, 'G': 7, 'G#': 8, 'Ab': 8, 'A': 9, 'A#': 10, 'Bb': 10, 'B': 11\n",
    "    }\n",
    "    if isinstance(note, str):  # if note is a string128N\n",
    "        if note.isdigit():  # if note is a number string\n",
    "            return int(note)\n",
    "        else:  # if note is a note string\n",
    "            # Separate the pitch and the octave (e.g. 'E5' -> ('E', '5'))\n",
    "            pitch, octave = note[:-1], note[-1]\n",
    "            # Replace '-' with 'b' in the pitch\n",
    "            pitch = pitch.replace('-', 'b')\n",
    "            # Calculate the MIDI number\n",
    "            midi_num = note_dict[pitch] + (int(octave) + 1) * 12\n",
    "            return midi_num\n",
    "    elif isinstance(note, int):  # if note is an integer\n",
    "        return note\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid note: {note}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mido\n",
    "\n",
    "def convert_to_midi(prediction_output, path):\n",
    "    mid = mido.MidiFile()\n",
    "    track = mido.MidiTrack()\n",
    "    mid.tracks.append(track)\n",
    "\n",
    "    for pattern in prediction_output:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            for current_note in notes_in_chord:\n",
    "                cn = note_to_int(current_note)\n",
    "                # Create note_on and note_off events\n",
    "                track.append(mido.Message('note_on', note=cn, velocity=64, time=0))\n",
    "                track.append(mido.Message('note_off', note=cn, velocity=64, time=480))  # Assuming note length of 480 ticks\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            cn = note_to_int(pattern)\n",
    "            # Create note_on and note_off events\n",
    "            track.append(mido.Message('note_on', note=cn, velocity=64, time=0))\n",
    "            track.append(mido.Message('note_off', note=cn, velocity=64, time=480))  # Assuming note length of 480 ticks\n",
    "\n",
    "    mid.save(path)\n",
    "    print(f\"MIDI file saved at: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking the seed tune as follows:\n",
      "[156 233 175   6 142 230  65 230 156 191 191  39  67 215 255  29  57  91\n",
      " 230 211 215  44  39 155 131 263   8 211 211 156  67 143]\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music0.midi\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music0.midi\n",
      "Taking the seed tune as follows:\n",
      "[126 281  31 236 281 164  31 280   2  90  16 174  90  31 281   2 286  16\n",
      "  36 280 137 259  31 286  55  90  31 185 281 166  90  55]\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music1.midi\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music1.midi\n",
      "Taking the seed tune as follows:\n",
      "[219 287  34  84 186  84   1 192 155 171 155  41 218  66 235 235 125  91\n",
      " 269  82 122  82 122 211  46  46  57  13 122 142 290 142]\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music2.midi\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music2.midi\n",
      "Taking the seed tune as follows:\n",
      "[125 164 231  49  49 100 231 242 100 231 100 293 293 220 279  92 105 100\n",
      " 247   1 247 290 279 255  41 191 125 291 234   9 236 100]\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music3.midi\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music3.midi\n",
      "Taking the seed tune as follows:\n",
      "[294 102  63  63  25 294 102 102  73 294 294 102 102 294 294 229 102 294\n",
      " 102 294 102 102 294 102 294  10 242  10  10 159 281  10]\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music4.midi\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music4.midi\n",
      "Taking the seed tune as follows:\n",
      "[263  74 272 269  61 207 295 297 133 149 206 295 105  99   1 295 130 162\n",
      " 102  25 295 162  30  39 234 176  89 123  75 149 210 234]\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music5.midi\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music5.midi\n",
      "Taking the seed tune as follows:\n",
      "[194  30 144 249 176 171 144 201 126 249 144 249 176 144 201 144 249 144\n",
      "  71 126 269 126 201 144  71  70 144  71 144 201  70 176]\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music6.midi\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music6.midi\n",
      "Taking the seed tune as follows:\n",
      "[214 299 186 178 230  39 186  39 214  39 186  39 218 255 186 218  48 178\n",
      "  39 186  39 218  39 186 218  39  39 299 255   1  48 233]\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music7.midi\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music7.midi\n",
      "Taking the seed tune as follows:\n",
      "[232  63 144 176 144 271 287  84 168  12 263 175 157 271  84 168 271  84\n",
      " 168 263  12 175 157 271 263 168  12 117 124 157 271 263]\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music8.midi\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music8.midi\n",
      "Taking the seed tune as follows:\n",
      "[ 24 118 272 202 202  92 272 175 110 110 113 205 240  96 272  54  67 273\n",
      "  74 175  74  74 182  99  24 122 143  47 122 125  91 269]\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music9.midi\n",
      "MIDI file saved at: /home/admin1/Desktop/NEWLSTM/Output/music9.midi\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random  # Import the random module\n",
    "\n",
    "for j in range(10):\n",
    "    index = random.randint(0,len(x_test))\n",
    "    print(\"Taking the seed tune as follows:\")\n",
    "    print(x_test[index])\n",
    "    tune = x_test[index]\n",
    "    input = np.empty((1,32),dtype=int)\n",
    "    input[0] = tune\n",
    "    input = torch.from_numpy(input)\n",
    "    next_preds = 64\n",
    "    for i in range(next_preds):\n",
    "        #output = Net(input.to(device),input.shape[0])\n",
    "        output = model(input.to(device))\n",
    "        next_preds = np.argmax(output.cpu().detach().numpy(),axis=1)\n",
    "        input = input.cpu().detach().numpy()\n",
    "        input = torch.from_numpy(np.array([np.append(j,next_preds[ind]) \n",
    "                                                   for ind,j in enumerate(input)])[:,1:]) \n",
    "\n",
    "        tune = np.insert(tune,-1,next_preds[0])\n",
    "    tune = [unique_notes[i] for i in tune]\n",
    "    path = '/home/admin1/Desktop/NEWLSTM/Output/music'+str(j)+'.midi'\n",
    "    \n",
    "    # Check if the directory exists, if not, create it\n",
    "    directory = os.path.dirname(path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    # Try to write the MIDI file\n",
    "    try:\n",
    "        convert_to_midi(tune, path)\n",
    "        print(f\"MIDI file saved at: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write MIDI file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
